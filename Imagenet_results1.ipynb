{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Imagenet_results",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORHJFmNyazFofBSF0gjTLl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51d9293051f1426295bb1ef12826c43a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b072d657c4ea44caa9e336c3dd226e9c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_668de3824aff4c8194117be534b1d3d0",
              "IPY_MODEL_1c1bd1c5ac61455f9cb89faa0d534e6a"
            ]
          }
        },
        "b072d657c4ea44caa9e336c3dd226e9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "668de3824aff4c8194117be534b1d3d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_775a42981f8244dc99d43466e34a0f40",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b642d95f56774a1a84ed9929bc5860f3"
          }
        },
        "1c1bd1c5ac61455f9cb89faa0d534e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_334da3b78204416e8880d198b629979b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [12:44&lt;00:00, 61.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee692694fb9f4f0c90067b71677206df"
          }
        },
        "775a42981f8244dc99d43466e34a0f40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b642d95f56774a1a84ed9929bc5860f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "334da3b78204416e8880d198b629979b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee692694fb9f4f0c90067b71677206df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r0cketr1kky/Angular/blob/master/Imagenet_results1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S40rGqPfGGk",
        "colab_type": "code",
        "outputId": "bb95d839-db59-4f00-f4bd-32ea4638627c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvuFJ6AZfdq1",
        "colab_type": "code",
        "outputId": "971130b7-b4bc-4f8d-e51d-3537ab4ca9bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI4EKwwpfuee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF6Hdm68fnPk",
        "colab_type": "code",
        "outputId": "2727db7e-8a0a-4617-f8f6-f0be7b6b6190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "file_path = '/gdrive/My Drive/test_data/'\n",
        "f = os.listdir(file_path)\n",
        "len(f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFK-aid0ftUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#AWESOME WE HAVE 2000 IMAGES!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SMV2cikgEvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter\n",
        "import matplotlib.cm as mpl_color_map\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "def convert_to_grayscale(im_as_arr):\n",
        "    \"\"\"\n",
        "        Converts 3d image to grayscale\n",
        "    Args:\n",
        "        im_as_arr (numpy arr): RGB image with shape (D,W,H)\n",
        "    returns:\n",
        "        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n",
        "    \"\"\"\n",
        "    grayscale_im = np.sum(np.abs(im_as_arr), axis=0)\n",
        "    im_max = np.percentile(grayscale_im, 99)\n",
        "    im_min = np.min(grayscale_im)\n",
        "    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n",
        "    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n",
        "    return grayscale_im\n",
        "\n",
        "\n",
        "def save_gradient_images(gradient, file_name):\n",
        "    \"\"\"\n",
        "        Exports the original gradient image\n",
        "    Args:\n",
        "        gradient (np arr): Numpy array of the gradient with shape (3, 224, 224)\n",
        "        file_name (str): File name to be exported\n",
        "    \"\"\"\n",
        "    if not os.path.exists('../results'):\n",
        "        os.makedirs('../results')\n",
        "    # Normalize\n",
        "    gradient = gradient - gradient.min()\n",
        "    gradient /= gradient.max()\n",
        "    # Save image\n",
        "    path_to_file = os.path.join('../results', file_name + '.jpg')\n",
        "    save_image1(gradient, path_to_file)\n",
        "\n",
        "\n",
        "def save_class_activation_images(org_img, activation_map):\n",
        "    \"\"\"\n",
        "        Saves cam activation map and activation map on the original image\n",
        "    Args:\n",
        "        org_img (PIL img): Original image\n",
        "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
        "    \"\"\"\n",
        "    if not os.path.exists('../results'):\n",
        "        os.makedirs('../results')\n",
        "    # Grayscale activation map\n",
        "    heatmap, heatmap_on_image = apply_colormap_on_image(org_img, activation_map, 'hsv')\n",
        "    # Save colored heatmap\n",
        "    path_to_file = os.path.join('../results', '/_Cam_Heatmap.png')\n",
        "    save_image1(heatmap, path_to_file)\n",
        "    # Save heatmap on iamge\n",
        "    path_to_file = os.path.join('../results', '/_Cam_On_Image.png')\n",
        "    save_image1(heatmap_on_image, path_to_file)\n",
        "    # SAve grayscale heatmap\n",
        "    path_to_file = os.path.join('../results', '/_Cam_Grayscale.png')\n",
        "    save_image1(activation_map, path_to_file)\n",
        "\n",
        "\n",
        "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
        "    \"\"\"\n",
        "        Apply heatmap on image\n",
        "    Args:\n",
        "        org_img (PIL img): Original image\n",
        "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
        "        colormap_name (str): Name of the colormap\n",
        "    \"\"\"\n",
        "    # Get colormap\n",
        "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
        "    no_trans_heatmap = color_map(activation)\n",
        "    # Change alpha channel in colormap to make sure original image is displayed\n",
        "    heatmap = copy.copy(no_trans_heatmap)\n",
        "    heatmap[:, :, 3] = 0.4\n",
        "    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n",
        "    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n",
        "\n",
        "    # Apply heatmap on iamge\n",
        "    heatmap_on_image = Image.new(\"RGBA\", org_im.size)\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert('RGBA'))\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
        "    return no_trans_heatmap, heatmap_on_image\n",
        "\n",
        "\n",
        "def format_np_output(np_arr):\n",
        "    \"\"\"\n",
        "        This is a (kind of) bandaid fix to streamline saving procedure.\n",
        "        It converts all the outputs to the same format which is 3xWxH\n",
        "        with using sucecssive if clauses.\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n",
        "    \"\"\"\n",
        "    # Phase/Case 1: The np arr only has 2 dimensions\n",
        "    # Result: Add a dimension at the beginning\n",
        "    if len(np_arr.shape) == 2:\n",
        "        np_arr = np.expand_dims(np_arr, axis=0)\n",
        "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
        "    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n",
        "    if np_arr.shape[0] == 1:\n",
        "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
        "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
        "    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n",
        "    if np_arr.shape[0] == 3:\n",
        "        np_arr = np_arr.transpose(1, 2, 0)\n",
        "    # Phase/Case 4: NP arr is normalized between 0-1\n",
        "    # Result: Multiply with 255 and change type to make it saveable by PIL\n",
        "    if np.max(np_arr) <= 1:\n",
        "        np_arr = (np_arr*255).astype(np.uint8)\n",
        "    return np_arr\n",
        "\n",
        "\n",
        "def save_image1(im, path):\n",
        "    \"\"\"\n",
        "        Saves a numpy matrix or PIL image as an image\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape DxWxH\n",
        "        path (str): Path to the image\n",
        "    \"\"\"\n",
        "    if isinstance(im, (np.ndarray, np.generic)):\n",
        "        im = format_np_output(im)\n",
        "        im = Image.fromarray(im)\n",
        "    im.save(path)\n",
        "\n",
        "\n",
        "def preprocess_image(pil_im, resize_im=True):\n",
        "    \"\"\"\n",
        "        Processes image for CNNs\n",
        "    Args:\n",
        "        PIL_img (PIL_img): PIL Image or numpy array to process\n",
        "        resize_im (bool): Resize to 224 or not\n",
        "    returns:\n",
        "        im_as_var (torch variable): Variable that contains processed float tensor\n",
        "    \"\"\"\n",
        "    # mean and std list for channels (Imagenet)\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    #ensure or transform incoming image to PIL image\n",
        "    if type(pil_im) != Image.Image:\n",
        "        try:\n",
        "            pil_im = Image.fromarray(pil_im)\n",
        "        except Exception as e:\n",
        "            print(\"could not transform PIL_img to a PIL Image object. Please check input.\")\n",
        "\n",
        "    # Resize image\n",
        "    if resize_im:\n",
        "        pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n",
        "\n",
        "    im_as_arr = np.float32(pil_im)\n",
        "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
        "    # Normalize the channels\n",
        "    for channel, _ in enumerate(im_as_arr):\n",
        "        im_as_arr[channel] /= 255\n",
        "        im_as_arr[channel] -= mean[channel]\n",
        "        im_as_arr[channel] /= std[channel]\n",
        "    # Convert to float tensor\n",
        "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
        "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
        "    im_as_ten.unsqueeze_(0)\n",
        "    # Convert to Pytorch variable\n",
        "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
        "    return im_as_var\n",
        "\n",
        "\n",
        "def recreate_image(im_as_var):\n",
        "    \"\"\"\n",
        "        Recreates images from a torch variable, sort of reverse preprocessing\n",
        "    Args:\n",
        "        im_as_var (torch variable): Image to recreate\n",
        "    returns:\n",
        "        recreated_im (numpy arr): Recreated image in array\n",
        "    \"\"\"\n",
        "    reverse_mean = [-0.485, -0.456, -0.406]\n",
        "    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n",
        "    recreated_im = copy.copy(im_as_var.data.numpy()[0])\n",
        "    for c in range(3):\n",
        "        recreated_im[c] /= reverse_std[c]\n",
        "        recreated_im[c] -= reverse_mean[c]\n",
        "    recreated_im[recreated_im > 1] = 1\n",
        "    recreated_im[recreated_im < 0] = 0\n",
        "    recreated_im = np.round(recreated_im * 255)\n",
        "\n",
        "    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n",
        "    return recreated_im\n",
        "\n",
        "\n",
        "def get_positive_negative_saliency(gradient):\n",
        "    \"\"\"\n",
        "        Generates positive and negative saliency maps based on the gradient\n",
        "    Args:\n",
        "        gradient (numpy arr): Gradient of the operation to visualize\n",
        "    returns:\n",
        "        pos_saliency ( )\n",
        "    \"\"\"\n",
        "    pos_saliency = (np.maximum(0, gradient) / gradient.max())\n",
        "    neg_saliency = (np.maximum(0, -gradient) / -gradient.min())\n",
        "    return pos_saliency, neg_saliency\n",
        "\n",
        "\n",
        "def get_example_params(example_index):\n",
        "    \"\"\"\n",
        "        Gets used variables for almost all visualizations, like the image, model etc.\n",
        "    Args:\n",
        "        example_index (int): Image id to use from examples\n",
        "    returns:\n",
        "        original_image (numpy arr): Original image read from the file\n",
        "        prep_img (numpy_arr): Processed image\n",
        "        target_class (int): Target class for the image\n",
        "        pretrained_model(Pytorch model): Model to use for the operations\n",
        "    \"\"\"\n",
        "    # Pick one of the examples\n",
        "    \n",
        "    # Read image\n",
        "    original_image = Image.open(img_path).convert('RGB')\n",
        "    # Process image\n",
        "    prep_img = preprocess_image(original_image)\n",
        "    # Define model\n",
        "    #pretrained_model = model\n",
        "    return (original_image,\n",
        "            prep_img,\n",
        "            target_class)\n",
        "    \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BaseProp(object):\n",
        "    \"\"\"\n",
        "        Base class for backpropagation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        \"\"\"Init\n",
        "        # Arguments:\n",
        "            model: torchvision.models. A pretrained model.\n",
        "            handle: list. Handle list that register a hook function.\n",
        "            relu_outputs: list. Forward output after relu.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.handle = []\n",
        "        self.relu_outputs = []\n",
        "\n",
        "    def _register_conv_hook(self):\n",
        "\n",
        "        \"\"\"\n",
        "            Register hook function to save gradient w.r.t input image.\n",
        "        \"\"\"\n",
        "\n",
        "        def _record_gradients(module, grad_in, grad_out):\n",
        "                self.gradients = grad_in[0]\n",
        "\n",
        "        for _, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.modules.conv.Conv2d) and module.in_channels == 3:\n",
        "                backward_handle = module.register_backward_hook(_record_gradients)\n",
        "                self.handle.append(backward_handle)\n",
        "\n",
        "    def _register_relu_hooks(self):\n",
        "\n",
        "        \"\"\"\n",
        "            Register hook function to save forward and backward relu result.\n",
        "        \"\"\"\n",
        "\n",
        "        # Save forward propagation output of the ReLU layer\n",
        "        def _record_output(module, input_, output):\n",
        "            self.relu_outputs.append(output)\n",
        "\n",
        "        def _clip_gradients(module, grad_in, grad_out):\n",
        "            # keep positive forward propagation output\n",
        "            relu_output = self.relu_outputs.pop()\n",
        "            relu_output[relu_output > 0] = 1\n",
        "\n",
        "            # keep positive backward propagation gradient\n",
        "            positive_grad_out = torch.clamp(grad_out[0], min=0.0)\n",
        "\n",
        "            # generate modified guided gradient\n",
        "            modified_grad_out = positive_grad_out * relu_output\n",
        "\n",
        "            return (modified_grad_out, )\n",
        "\n",
        "        for _, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.ReLU):\n",
        "                forward_handle = module.register_forward_hook(_record_output)\n",
        "                backward_handle = module.register_backward_hook(_clip_gradients)\n",
        "                self.handle.append(forward_handle)\n",
        "                self.handle.append(backward_handle)\n",
        "\n",
        "class VanillaBackprop():\n",
        "    \"\"\"\n",
        "        Produces gradients generated with vanilla back propagation from the image\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        # Put model in evaluation mode\n",
        "        self.model.eval()\n",
        "        # Hook the first layer to get the gradient\n",
        "        self.hook_layers()\n",
        "\n",
        "    def hook_layers(self):\n",
        "        def hook_function(module, grad_in, grad_out):\n",
        "            self.gradients = grad_in[0]\n",
        "\n",
        "        # Register hook to the first layer\n",
        "        first_layer = list(self.model.features._modules.items())[0][1]\n",
        "        first_layer.register_backward_hook(hook_function)\n",
        "\n",
        "    def generate_gradients(self, input_image, target_class):\n",
        "        # Forward\n",
        "        model_output = self.model(input_image)\n",
        "        # Zero grads\n",
        "        self.model.zero_grad()\n",
        "        # Target for backprop\n",
        "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        # Backward pass\n",
        "        model_output.backward(gradient=one_hot_output)\n",
        "        # Convert Pytorch variable to numpy array\n",
        "        # [0] to get rid of the first channel (1,3,224,224)\n",
        "        gradients_as_arr = self.gradients.data.numpy()[0]\n",
        "        return gradients_as_arr\n",
        "\n",
        "class Backprop(BaseProp):\n",
        "\n",
        "    \"\"\" Generates vanilla or guided backprop gradients of a target class output w.r.t. an input image.\n",
        "        # Arguments:\n",
        "            model: torchvision.models. A pretrained model.\n",
        "            guided: bool. If True, perform guided backpropagation. Defaults to False.\n",
        "        # Return:\n",
        "            Backprop Class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, guided=False):\n",
        "        super().__init__(model)\n",
        "        self.model.eval()\n",
        "        self.guided = guided\n",
        "        self.gradients = None\n",
        "        self._register_conv_hook()\n",
        "\n",
        "    def calculate_gradients(self,\n",
        "                            input_,\n",
        "                            target_class=None,\n",
        "                            take_max=False,\n",
        "                            use_gpu=False):\n",
        "\n",
        "        \"\"\" Calculate gradient.\n",
        "            # Arguments\n",
        "                input_: torch.Tensor. Preprocessed image with shape (1, C, H, W).\n",
        "                target_class: int. Index of target class. Default to None and use the prediction result as target class.\n",
        "                take_max: bool. Take the maximum across colour channels. Defaults to False.\n",
        "                use_gpu. bool. Use GPU or not. Defaults to False.\n",
        "            # Return:\n",
        "                Gradient (torch.Tensor) with shape (C, H, W). If take max is True, with shape (1, H, W).\n",
        "        \"\"\"\n",
        "\n",
        "        if self.guided:\n",
        "            self._register_relu_hooks()\n",
        "\n",
        "        # Create a empty tensor to save gradients\n",
        "        self.gradients = torch.zeros(input_.shape)\n",
        "\n",
        "        output = self.model(input_)\n",
        "\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        if output.shape == torch.Size([1]):\n",
        "            target = None\n",
        "        else:\n",
        "            pred_class = output.argmax().item()\n",
        "\n",
        "            # Create a Tensor with zero elements, set the element at pred class index to be 1\n",
        "            target = torch.zeros(output.shape)\n",
        "\n",
        "            # If target class is None, calculate gradient of predicted class.\n",
        "            if target_class is None:\n",
        "                target[0][pred_class] = 1\n",
        "            else:\n",
        "                target[0][target_class] = 1\n",
        "\n",
        "            \n",
        "        # Calculate gradients w.r.t. input image\n",
        "        output.backward(gradient=target)\n",
        "\n",
        "        gradients = self.gradients.detach().cpu()[0]\n",
        "\n",
        "        if take_max:\n",
        "            gradients = gradients.max(dim=0, keepdim=True)[0]\n",
        "\n",
        "        for module in self.handle:\n",
        "            module.remove()\n",
        "        gradients = gradients.numpy()\n",
        "        return gradients\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# from guided_backprop import GuidedBackprop  # To use with guided backprop\n",
        "\n",
        "\n",
        "def generate_smooth_grad(Backprop, prep_img, target_class, param_n, param_sigma_multiplier):\n",
        "    \"\"\"\n",
        "        Generates smooth gradients of given Backprop type. You can use this with both vanilla\n",
        "        and guided backprop\n",
        "    Args:\n",
        "        Backprop (class): Backprop type\n",
        "        prep_img (torch Variable): preprocessed image\n",
        "        target_class (int): target class of imagenet\n",
        "        param_n (int): Amount of images used to smooth gradient\n",
        "        param_sigma_multiplier (int): Sigma multiplier when calculating std of noise\n",
        "    \"\"\"\n",
        "    # Generate an empty image/matrix\n",
        "    smooth_grad = np.zeros(prep_img.size()[1:])\n",
        "\n",
        "    mean = 0\n",
        "    sigma = param_sigma_multiplier / (torch.max(prep_img) - torch.min(prep_img)).item()\n",
        "    \n",
        "    for x in range(param_n):\n",
        "        # Generate noise\n",
        "        noise = Variable(prep_img.data.new(prep_img.size()).normal_(mean, sigma**2))\n",
        "        # Add noise to the image\n",
        "        noisy_img = prep_img + noise\n",
        "        # Calculate gradients\n",
        "        vanilla_grads = Backprop.calculate_gradients(noisy_img, target_class)\n",
        "        # Add gradients to smooth_grad\n",
        "        smooth_grad = smooth_grad + vanilla_grads\n",
        "    # Average it out\n",
        "    smooth_grad = smooth_grad / param_n\n",
        "    return smooth_grad\n",
        "\n",
        "def process_image(image_path):\n",
        "    \"\"\"Process an image path into a PyTorch tensor\"\"\"\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    # Resize\n",
        "    img = image.resize((256, 256))\n",
        "\n",
        "    # Center crop\n",
        "    width = 256\n",
        "    height = 256\n",
        "    new_width = 224\n",
        "    new_height = 224\n",
        "\n",
        "    left = (width - new_width) / 2\n",
        "    top = (height - new_height) / 2\n",
        "    right = (width + new_width) / 2\n",
        "    bottom = (height + new_height) / 2\n",
        "    img = img.crop((left, top, right, bottom))\n",
        "\n",
        "    # Convert to numpy, transpose color dimension and normalize\n",
        "    img = np.array(img).transpose((2, 0, 1)) / 256\n",
        "\n",
        "    # Standardization\n",
        "    means = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
        "    stds = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
        "\n",
        "    img = img - means\n",
        "    img = img / stds\n",
        "\n",
        "    img_tensor = torch.Tensor(img)\n",
        "\n",
        "    return img_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVSswMZPgJeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"flashtorch.utils\n",
        "\n",
        "This module provides utility functions for image handling and tensor\n",
        "transformation.\n",
        "\n",
        "\"\"\"\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.models\n",
        "\n",
        "def load_image(image_path):\n",
        "    \"\"\"Loads image as a PIL RGB image.\n",
        "\n",
        "        Args:\n",
        "            - **image_path (str) - **: A path to the image\n",
        "\n",
        "        Returns:\n",
        "            An instance of PIL.Image.Image in RGB\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return Image.open(image_path).convert('RGB')\n",
        "\n",
        "\n",
        "def apply_transforms(image, size=224):\n",
        "    \"\"\"Transforms a PIL image to torch.Tensor.\n",
        "\n",
        "    Applies a series of tranformations on PIL image including a conversion\n",
        "    to a tensor. The returned tensor has a shape of :math:`(N, C, H, W)` and\n",
        "    is ready to be used as an input to neural networks.\n",
        "\n",
        "    First the image is resized to 256, then cropped to 224. The `means` and\n",
        "    `stds` for normalisation are taken from numbers used in ImageNet, as\n",
        "    currently developing the package for visualizing pre-trained models.\n",
        "\n",
        "    The plan is to to expand this to handle custom size/mean/std.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image.Image or numpy array)\n",
        "        size (int, optional, default=224): Desired size (width/height) of the\n",
        "            output tensor\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(C, H, W)` for numpy array\n",
        "        Output: :math:`(N, C, H, W)`\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor (torch.float32): Transformed image tensor\n",
        "\n",
        "    Note:\n",
        "        Symbols used to describe dimensions:\n",
        "            - N: number of images in a batch\n",
        "            - C: number of channels\n",
        "            - H: height of the image\n",
        "            - W: width of the image\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(image, Image.Image):\n",
        "        image = F.to_pil_image(image)\n",
        "\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.CenterCrop(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds)\n",
        "    ])\n",
        "\n",
        "    tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    tensor.requires_grad = True\n",
        "\n",
        "    return tensor\n",
        "\n",
        "def apply_transforms_v0(image, size=224):\n",
        "    \"\"\"Transforms a PIL image to torch.Tensor.\n",
        "\n",
        "    Applies a series of tranformations on PIL image including a conversion\n",
        "    to a tensor. The returned tensor has a shape of :math:`(N, C, H, W)` and\n",
        "    is ready to be used as an input to neural networks.\n",
        "\n",
        "    First the image is resized to 256, then cropped to 224. The `means` and\n",
        "    `stds` for normalisation are taken from numbers used in ImageNet, as\n",
        "    currently developing the package for visualizing pre-trained models.\n",
        "\n",
        "    The plan is to to expand this to handle custom size/mean/std.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image.Image or numpy array)\n",
        "        size (int, optional, default=224): Desired size (width/height) of the\n",
        "            output tensor\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(C, H, W)` for numpy array\n",
        "        Output: :math:`(N, C, H, W)`\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor (torch.float32): Transformed image tensor\n",
        "\n",
        "    Note:\n",
        "        Symbols used to describe dimensions:\n",
        "            - N: number of images in a batch\n",
        "            - C: number of channels\n",
        "            - H: height of the image\n",
        "            - W: width of the image\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(image, Image.Image):\n",
        "        image = F.to_pil_image(image)\n",
        "\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.CenterCrop(size),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    tensor.requires_grad = True\n",
        "\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def denormalize(tensor):\n",
        "    \"\"\"Reverses the normalisation on a tensor.\n",
        "\n",
        "    Performs a reverse operation on a tensor, so the pixel value range is\n",
        "    between 0 and 1. Useful for when plotting a tensor into an image.\n",
        "\n",
        "    Normalisation: (image - mean) / std\n",
        "    Denormalisation: image * std + mean\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor, dtype=torch.float32): Normalized image tensor\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(N, C, H, W)`\n",
        "        Output: :math:`(N, C, H, W)` (same shape as input)\n",
        "\n",
        "    Return:\n",
        "        torch.Tensor (torch.float32): Demornalised image tensor with pixel\n",
        "            values between [0, 1]\n",
        "\n",
        "    Note:\n",
        "        Symbols used to describe dimensions:\n",
        "            - N: number of images in a batch\n",
        "            - C: number of channels\n",
        "            - H: height of the image\n",
        "            - W: width of the image\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    denormalized = tensor.clone()\n",
        "\n",
        "    for channel, mean, std in zip(denormalized[0], means, stds):\n",
        "        channel.mul_(std).add_(mean)\n",
        "\n",
        "    return denormalized\n",
        "\n",
        "\n",
        "def standardize_and_clip(tensor, min_value=0.0, max_value=1.0):\n",
        "    \"\"\"Standardizes and clips input tensor.\n",
        "\n",
        "    Standardize the input tensor (mean = 0.0, std = 1.0), ensures std is 0.1\n",
        "    and clips it to values between min/max (default: 0.0/1.0).\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor):\n",
        "        min_value (float, optional, default=0.0)\n",
        "        max_value (float, optional, default=1.0)\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(C, H, W)`\n",
        "        Output: Same as the input\n",
        "\n",
        "    Return:\n",
        "        torch.Tensor (torch.float32): Normalised tensor with values between\n",
        "            [min_value, max_value]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    tensor = tensor.detach().cpu()\n",
        "\n",
        "    mean = tensor.mean()\n",
        "    std = tensor.std()\n",
        "    if std == 0:\n",
        "        std += 1e-7\n",
        "\n",
        "    standardized = tensor.sub(mean).div(std).mul(0.1)\n",
        "    clipped = standardized.add(0.5).clamp(min_value, max_value)\n",
        "\n",
        "    return clipped\n",
        "\n",
        "\n",
        "def format_for_plotting(tensor):\n",
        "    \"\"\"Formats the shape of tensor for plotting.\n",
        "\n",
        "    Tensors typically have a shape of :math:`(N, C, H, W)` or :math:`(C, H, W)`\n",
        "    which is not suitable for plotting as images. This function formats an\n",
        "    input tensor :math:`(H, W, C)` for RGB and :math:`(H, W)` for mono-channel\n",
        "    data.\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor, torch.float32): Image tensor\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`\n",
        "        Output: :math:`(H, W, C)` or :math:`(H, W)`, respectively\n",
        "\n",
        "    Return:\n",
        "        torch.Tensor (torch.float32): Formatted image tensor (detached)\n",
        "\n",
        "    Note:\n",
        "        Symbols used to describe dimensions:\n",
        "            - N: number of images in a batch\n",
        "            - C: number of channels\n",
        "            - H: height of the image\n",
        "            - W: width of the image\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    has_batch_dimension = len(tensor.shape) == 4\n",
        "    formatted = tensor.clone()\n",
        "\n",
        "    if has_batch_dimension:\n",
        "        formatted = tensor.squeeze(0)\n",
        "\n",
        "    if formatted.shape[0] == 1:\n",
        "        return formatted.squeeze(0).detach()\n",
        "    else:\n",
        "        return formatted.permute(1, 2, 0).detach()\n",
        "\n",
        "\n",
        "def visualize(input_, gradients, save_path=None, cmap='viridis', alpha=0.7):\n",
        "\n",
        "    \"\"\" Method to plot the explanation.\n",
        "\n",
        "        # Arguments\n",
        "            input_: Tensor. Original image.\n",
        "            gradients: Tensor. Saliency map result.\n",
        "            save_path: String. Defaults to None.\n",
        "            cmap: Defaults to be 'viridis'.\n",
        "            alpha: Defaults to be 0.7.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    input_ = format_for_plotting(denormalize(input_))\n",
        "    gradients = format_for_plotting(standardize_and_clip(gradients))\n",
        "\n",
        "    subplots = [\n",
        "        ('Input image', [(input_, None, None)]),\n",
        "        ('Saliency map across RGB channels', [(gradients, None, None)]),\n",
        "        ('Overlay', [(input_, None, None), (gradients, cmap, alpha)])\n",
        "    ]\n",
        "\n",
        "    num_subplots = len(subplots)\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 3))\n",
        "\n",
        "    for i, (title, images) in enumerate(subplots):\n",
        "        ax = fig.add_subplot(1, num_subplots, i + 1)\n",
        "        ax.set_axis_off()\n",
        "\n",
        "        for image, cmap, alpha in images:\n",
        "            ax.imshow(image, cmap=cmap, alpha=alpha)\n",
        "\n",
        "        ax.set_title(title)\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "\n",
        "def basic_visualize(input_, gradients, save_path=None, weight=None, cmap='viridis', alpha=0.7):\n",
        "\n",
        "    \"\"\" Method to plot the explanation.\n",
        "\n",
        "        # Arguments\n",
        "            input_: Tensor. Original image.\n",
        "            gradients: Tensor. Saliency map result.\n",
        "            save_path: String. Defaults to None.\n",
        "            cmap: Defaults to be 'viridis'.\n",
        "            alpha: Defaults to be 0.7.\n",
        "\n",
        "    \"\"\"\n",
        "    input_ = format_for_plotting(denormalize(input_))\n",
        "    gradients = format_for_plotting(standardize_and_clip(gradients))\n",
        "\n",
        "    subplots = [\n",
        "        ('Saliency map across RGB channels', [(gradients, None, None)]),\n",
        "        ('Overlay', [(input_, None, None), (gradients, cmap, alpha)])\n",
        "    ]\n",
        "\n",
        "    num_subplots = len(subplots)\n",
        "\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    count=0\n",
        "    for i, (title, images) in enumerate(subplots):\n",
        "        ax = fig.add_subplot(1, num_subplots, i + 1)\n",
        "        ax.set_axis_off()\n",
        "\n",
        "        for image, cmap, alpha in images:\n",
        "            print('hello {}'.format(count))\n",
        "            count+=1\n",
        "            ax.imshow(image, cmap=cmap, alpha=alpha)\n",
        "\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def find_resnet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find resnet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'conv1'\n",
        "            target_layer_name = 'layer1'\n",
        "            target_layer_name = 'layer1_basicblock0'\n",
        "            target_layer_name = 'layer1_basicblock0_relu'\n",
        "            target_layer_name = 'layer1_bottleneck0'\n",
        "            target_layer_name = 'layer1_bottleneck0_conv1'\n",
        "            target_layer_name = 'layer1_bottleneck0_downsample'\n",
        "            target_layer_name = 'layer1_bottleneck0_downsample_0'\n",
        "            target_layer_name = 'avgpool'\n",
        "            target_layer_name = 'fc'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'layer4'\n",
        "\n",
        "    if 'layer' in target_layer_name:\n",
        "        hierarchy = target_layer_name.split('_')\n",
        "        layer_num = int(hierarchy[0].lstrip('layer'))\n",
        "        if layer_num == 1:\n",
        "            target_layer = arch.layer1\n",
        "        elif layer_num == 2:\n",
        "            target_layer = arch.layer2\n",
        "        elif layer_num == 3:\n",
        "            target_layer = arch.layer3\n",
        "        elif layer_num == 4:\n",
        "            target_layer = arch.layer4\n",
        "        else:\n",
        "            raise ValueError('unknown layer : {}'.format(target_layer_name))\n",
        "\n",
        "        if len(hierarchy) >= 2:\n",
        "            bottleneck_num = int(hierarchy[1].lower().lstrip('bottleneck').lstrip('basicblock'))\n",
        "            target_layer = target_layer[bottleneck_num]\n",
        "\n",
        "        if len(hierarchy) >= 3:\n",
        "            target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "        if len(hierarchy) == 4:\n",
        "            target_layer = target_layer._modules[hierarchy[3]]\n",
        "\n",
        "    else:\n",
        "        target_layer = arch._modules[target_layer_name]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_densenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find densenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_transition1'\n",
        "            target_layer_name = 'features_transition1_norm'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n",
        "            target_layer_name = 'classifier'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) >= 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    if len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_vgg_layer(arch, target_layer_name):\n",
        "    \"\"\"Find vgg layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_42'\n",
        "            target_layer_name = 'classifier'\n",
        "            target_layer_name = 'classifier_0'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "\n",
        "    if len(hierarchy) >= 1:\n",
        "        target_layer = arch.features\n",
        "\n",
        "    if len(hierarchy) == 2:\n",
        "        target_layer = target_layer[int(hierarchy[1])]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_alexnet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find alexnet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "              saliency_map2 = torch.unsqueeze(activations[:, i, :, :], 1)\n",
        "              \n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_0'\n",
        "            target_layer_name = 'classifier'\n",
        "            target_layer_name = 'classifier_0'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features_29'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "\n",
        "    if len(hierarchy) >= 1:\n",
        "        target_layer = arch.features\n",
        "\n",
        "    if len(hierarchy) == 2:\n",
        "        target_layer = target_layer[int(hierarchy[1])]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_squeezenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find squeezenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "        Args:\n",
        "            - **arch - **: default torchvision densenet models\n",
        "            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n",
        "                target_layer_name = 'features_12'\n",
        "                target_layer_name = 'features_12_expand3x3'\n",
        "                target_layer_name = 'features_12_expand3x3_activation'\n",
        "\n",
        "        Return:\n",
        "            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_googlenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find squeezenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "        Args:\n",
        "            - **arch - **: default torchvision googlenet models\n",
        "            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n",
        "                target_layer_name = 'inception5b'\n",
        "\n",
        "        Return:\n",
        "            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_mobilenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find mobilenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "        Args:\n",
        "            - **arch - **: default torchvision googlenet models\n",
        "            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n",
        "                target_layer_name = 'features'\n",
        "\n",
        "        Return:\n",
        "            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_shufflenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find mobilenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "        Args:\n",
        "            - **arch - **: default torchvision googlenet models\n",
        "            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n",
        "                target_layer_name = 'conv5'\n",
        "\n",
        "        Return:\n",
        "            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_layer(arch, target_layer_name):\n",
        "    \"\"\"Find target layer to calculate CAM.\n",
        "\n",
        "        : Args:\n",
        "            - **arch - **: Self-defined architecture.\n",
        "            - **target_layer_name - ** (str): Name of target class.\n",
        "\n",
        "        : Return:\n",
        "            - **target_layer - **: Found layer. This layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "\n",
        "    if target_layer_name.split('_') not in arch._modules.keys():\n",
        "        raise Exception(\"Invalid target layer name.\")\n",
        "    target_layer = arch._modules[target_layer_name]\n",
        "    return target_layer\n",
        "'''\n",
        "Part of code borrows from https://github.com/1Konny/gradcam_plus_plus-pytorch\n",
        "'''\n",
        "\n",
        "import torch\n",
        "\n",
        "class BaseCAM(object):\n",
        "    \"\"\" Base class for Class activation mapping.\n",
        "        : Args\n",
        "            - **model_dict -** : Dict. Has format as dict(type='vgg', arch=torchvision.models.vgg16(pretrained=True),\n",
        "            layer_name='features',input_size=(224, 224)).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        model_type = model_dict['type']\n",
        "        layer_name = model_dict['layer_name']\n",
        "        \n",
        "        self.model_arch = model_dict['arch']\n",
        "        self.model_arch.eval()\n",
        "        #if torch.cuda.is_available():\n",
        "        #  self.model_arch.cuda()\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            #if torch.cuda.is_available():\n",
        "            #  self.gradients['value'] = grad_output[0].cuda()\n",
        "            \n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            \n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            #if torch.cuda.is_available():\n",
        "            #  self.activations['value'] = output.cuda()\n",
        "            self.activations['value'] = output\n",
        "            \n",
        "            return None\n",
        "\n",
        "        if 'vgg' in model_type.lower():\n",
        "            self.target_layer = find_vgg_layer(self.model_arch, layer_name)\n",
        "        elif 'resnet' in model_type.lower():\n",
        "            self.target_layer = find_resnet_layer(self.model_arch, layer_name)\n",
        "        elif 'densenet' in model_type.lower():\n",
        "            self.target_layer = find_densenet_layer(self.model_arch, layer_name)\n",
        "        elif 'alexnet' in model_type.lower():\n",
        "            self.target_layer = find_alexnet_layer(self.model_arch, layer_name)\n",
        "        elif 'squeezenet' in model_type.lower():\n",
        "            self.target_layer = find_squeezenet_layer(self.model_arch, layer_name)\n",
        "        elif 'googlenet' in model_type.lower():\n",
        "            self.target_layer = find_googlenet_layer(self.model_arch, layer_name)\n",
        "        elif 'shufflenet' in model_type.lower():\n",
        "            self.target_layer = find_shufflenet_layer(self.model_arch, layer_name)\n",
        "        elif 'mobilenet' in model_type.lower():\n",
        "            self.target_layer = find_mobilenet_layer(self.model_arch, layer_name)\n",
        "        else:\n",
        "            self.target_layer = find_layer(self.model_arch, layer_name)\n",
        "\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        return None\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GradCAM(BaseCAM):\n",
        "    \"\"\"\n",
        "        GradCAM, inherit from BaseCAM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        super().__init__(model_dict)\n",
        "\n",
        "    def forward(self, input_, class_idx=None, retain_graph=False):\n",
        "        \"\"\"Generates GradCAM result.\n",
        "\n",
        "        # Arguments\n",
        "            input_: torch.Tensor. Preprocessed image with shape (1, C, H, W).\n",
        "            class_idx: int. Index of target class. Defaults to be index of predicted class.\n",
        "\n",
        "        # Return\n",
        "            Result of GradCAM (torch.Tensor) with shape (1, H, W).\n",
        "        \"\"\"\n",
        "\n",
        "        b, c, h, w = input_.size()\n",
        "        logit = self.model_arch(input_)\n",
        "\n",
        "        if class_idx is None:\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "\n",
        "        \n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "\n",
        "        return saliency_map\n",
        "\n",
        "\n",
        "class ScoreCAM(BaseCAM):\n",
        "\n",
        "    \"\"\"\n",
        "        ScoreCAM, inherit from BaseCAM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        super().__init__(model_dict)\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        b, c, h, w = input.size()\n",
        "        \n",
        "        # predication on raw input\n",
        "        logit = self.model_arch(input).cuda()\n",
        "        \n",
        "        if class_idx is None:\n",
        "            predicted_class = logit.max(1)[-1]\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            predicted_class = torch.LongTensor([class_idx])\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "        \n",
        "        logit = F.softmax(logit)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          predicted_class= predicted_class.cuda()\n",
        "          score = score.cuda()\n",
        "          logit = logit.cuda()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        \n",
        "        score_saliency_map = torch.zeros((1, 1, h, w))\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          activations = activations.cuda()\n",
        "          score_saliency_map = score_saliency_map.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for i in range(k):\n",
        "\n",
        "              # upsampling\n",
        "              saliency_map = torch.unsqueeze(activations[:, i, :, :], 1)\n",
        "              saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "              \n",
        "              if saliency_map.max() == saliency_map.min():\n",
        "                continue\n",
        "              \n",
        "              # normalize to 0-1\n",
        "              norm_saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "              # how much increase if keeping the highlighted region\n",
        "              # predication on masked input\n",
        "              output = self.model_arch(input * norm_saliency_map)\n",
        "              output = F.softmax(output)\n",
        "              score = output[0][predicted_class]\n",
        "\n",
        "              score_saliency_map +=  score * saliency_map\n",
        "                \n",
        "        score_saliency_map = F.relu(score_saliency_map)\n",
        "        score_saliency_map_min, score_saliency_map_max = score_saliency_map.min(), score_saliency_map.max()\n",
        "\n",
        "        if score_saliency_map_min == score_saliency_map_max:\n",
        "            return None\n",
        "\n",
        "        score_saliency_map = (score_saliency_map - score_saliency_map_min).div(score_saliency_map_max - score_saliency_map_min).data\n",
        "\n",
        "        return score_saliency_map\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)\n",
        "\n",
        "class ScoreCAM2(BaseCAM):\n",
        "\n",
        "    \"\"\"\n",
        "        ScoreCAM, inherit from BaseCAM\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        super().__init__(model_dict)\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        b, c, h, w = input.size()\n",
        "        \n",
        "        # predication on raw input\n",
        "        logit = self.model_arch(input)\n",
        "        \n",
        "        if class_idx is None:\n",
        "            predicted_class = logit.max(1)[-1]\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            predicted_class = torch.LongTensor([class_idx])\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "        \n",
        "        logit = F.softmax(logit)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          predicted_class= predicted_class.cuda()\n",
        "          score = score.cuda()\n",
        "          logit = logit.cuda()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        activations = self.activations['value']\n",
        "        b1, k, u, v = activations.size()\n",
        "        \n",
        "        score_saliency_map = torch.zeros((1, 1, h, w))\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          activations = activations.cuda()\n",
        "          score_saliency_map = score_saliency_map.cuda()\n",
        "\n",
        "        mean = 0\n",
        "        param_n = 10\n",
        "        param_sigma_multiplier = 4\n",
        "        \n",
        "\n",
        "        with torch.no_grad():\n",
        "          for i in range(k):\n",
        "\n",
        "              # upsampling\n",
        "\n",
        "              saliency_map = torch.unsqueeze(activations[:, i, :, :], 1)\n",
        "\n",
        "              saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                \n",
        "              if saliency_map.max() == saliency_map.min():\n",
        "                continue\n",
        "\n",
        "              \n",
        "                # normalize to 0-1\n",
        "              norm_saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "              x = input * norm_saliency_map\n",
        "\n",
        "              if (torch.max(x) - torch.min(x)).item() == 0:\n",
        "                continue\n",
        "              else:\n",
        "                sigma = param_sigma_multiplier / (torch.max(x) - torch.min(x)).item()\n",
        "              \n",
        "              score_list = []\n",
        "              noisy_list = []\n",
        "              \n",
        "              for _ in range(param_n):\n",
        "\n",
        "                noise = Variable(x.data.new(x.size()).normal_(mean, sigma**2))\n",
        "\n",
        "                noisy_img = x + noise\n",
        "\n",
        "                noisy_list.append(noisy_img)\n",
        "               \n",
        "                output = self.model_arch(noisy_img)\n",
        "                output = F.softmax(output)\n",
        "                score = output[0][predicted_class]\n",
        "                score_list.append(score)\n",
        "              \n",
        "              score = sum(score_list) / len(score_list)\n",
        "              score_saliency_map +=  score * saliency_map\n",
        "                \n",
        "        score_saliency_map = F.relu(score_saliency_map)\n",
        "        score_saliency_map_min, score_saliency_map_max = score_saliency_map.min(), score_saliency_map.max()\n",
        "\n",
        "        if score_saliency_map_min == score_saliency_map_max:\n",
        "            return None\n",
        "\n",
        "        score_saliency_map = (score_saliency_map - score_saliency_map_min).div(score_saliency_map_max - score_saliency_map_min).data\n",
        "\n",
        "        return score_saliency_map\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeJa9iYfg6Xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQtZ9Nlhg-xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listscore = []\n",
        "listoci = []\n",
        "avg_drop = []\n",
        "avg_iic = []\n",
        "avg_drop_sc = []\n",
        "avg_iic_sc = []\n",
        "count1 = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22tkp45ehDYX",
        "colab_type": "code",
        "outputId": "44ea2252-cafa-4d62-e595-ae34347b1414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "51d9293051f1426295bb1ef12826c43a",
            "b072d657c4ea44caa9e336c3dd226e9c",
            "668de3824aff4c8194117be534b1d3d0",
            "1c1bd1c5ac61455f9cb89faa0d534e6a",
            "775a42981f8244dc99d43466e34a0f40",
            "b642d95f56774a1a84ed9929bc5860f3",
            "334da3b78204416e8880d198b629979b",
            "ee692694fb9f4f0c90067b71677206df"
          ]
        }
      },
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model = model.cuda()\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51d9293051f1426295bb1ef12826c43a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNyCuutShh6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_type = 'resnet'\n",
        "layer_name = 'layer4_basicblock1_conv2' # Last conv layer before average pooling layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3HiK7XyhLak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_activation_map(image_path):\n",
        "\n",
        "  from torchvision import transforms, datasets, models\n",
        "  import torch\n",
        "  from torch import optim, cuda\n",
        "  from torch.utils.data import DataLoader, sampler\n",
        "  import torch.nn as nn\n",
        "\n",
        "  import warnings\n",
        "  warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "  # Data science tools\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  import os\n",
        "\n",
        "\n",
        "  # Image manipulations\n",
        "  from PIL import Image\n",
        "  # Useful for examining network\n",
        "  from torchsummary import summary\n",
        "  # Timing utility\n",
        "  from timeit import default_timer as timer\n",
        "\n",
        "  # Visualizations\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "  plt.rcParams['font.size'] = 14\n",
        "\n",
        "  X = model.eval()\n",
        "\n",
        "  x_model_dict = dict(type=model_type, arch=X, layer_name=layer_name, input_size=(224, 224))\n",
        "\n",
        "  x_scorecam = ScoreCAM(x_model_dict)\n",
        "\n",
        "  input_image = load_image(image_path)\n",
        "  input_ = apply_transforms(input_image)\n",
        "  \n",
        "  if torch.cuda.is_available():\n",
        "    input_cuda = input_.cuda()\n",
        "  \n",
        "  predicted_class = X(input_cuda).max(1)[-1]\n",
        "\n",
        "  #print(input_cuda)\n",
        "  scorecam_map = x_scorecam(input_cuda)\n",
        "  print(scorecam_map)\n",
        "  if scorecam_map is None:\n",
        "    return\n",
        "  \n",
        "  basic_visualize(input_cuda.cpu(), scorecam_map.type(torch.FloatTensor).cpu())\n",
        "\n",
        "  x_scorecam1 = ScoreCAM2(x_model_dict)\n",
        "  \n",
        "  scorecam_map1 = x_scorecam1(input_cuda)\n",
        "  #print(scorecam_map1)\n",
        "\n",
        "  if scorecam_map is None:\n",
        "    return\n",
        "\n",
        "  basic_visualize(input_cuda.cpu(), scorecam_map1.type(torch.FloatTensor).cpu())    \n",
        "\n",
        "  mask_intensity = 50\n",
        "  print(\"ScoreCAMs completed\")\n",
        "\n",
        "  seg = scorecam_map1[0][0].cpu().numpy().astype(np.float32)\n",
        "  #plt.imshow(seg)\n",
        "  img = Image.fromarray(np.uint8(seg * 255) , 'L')\n",
        "  #plt.imshow(img)\n",
        "  img.save('./My Drive/saved_img.jpg')\n",
        "  arr = cv2.imread('./My Drive/saved_img.jpg',cv2.IMREAD_GRAYSCALE)\n",
        "  #print(arr)\n",
        "  mask =np.where(arr>mask_intensity,1,0)\n",
        "\n",
        "  Yci = X(input_cuda).max(1)[0]\n",
        "  print(Yci)\n",
        "  \n",
        "  img = Image.fromarray(np.uint8(mask * 255) , 'L')\n",
        "  img.save('./My Drive/saved_heloo.jpeg')\n",
        "  \n",
        "  im = load_image('./My Drive/saved_heloo.jpeg')\n",
        "  #print(im.shape)\n",
        "  m_m = apply_transforms(im)\n",
        "  m_m = m_m.cuda()\n",
        "\n",
        "  Oci = X(m_m).max(1)[0]\n",
        "  print(Oci)\n",
        "  \n",
        "  avg_drop.append(((max(0, Yci - Oci))/(Yci)))\n",
        "  print(((max(0, Yci - Oci))/(Yci)))\n",
        "\n",
        "  if Yci < Oci:\n",
        "    avg_iic.append(1)\n",
        "  else:\n",
        "    avg_iic.append(0)\n",
        "\n",
        "\n",
        "  seg = scorecam_map[0][0].cpu().numpy().astype(np.float32)\n",
        "  #plt.imshow(seg)\n",
        "  img = Image.fromarray(np.uint8(seg * 255) , 'L')\n",
        "  #plt.imshow(img)\n",
        "  img.save('./My Drive/saved_img.jpg')\n",
        "  arr = cv2.imread('./My Drive/saved_img.jpg',cv2.IMREAD_GRAYSCALE)\n",
        "  #print(arr)\n",
        "  mask =np.where(arr>mask_intensity,1,0)\n",
        "  img = Image.fromarray(np.uint8(mask * 255) , 'L')\n",
        "  img.save('./My Drive/saved_mask.jpg')\n",
        "  im = load_image('./My Drive/saved_mask.jpg')\n",
        "  #print(im.shape)\n",
        "  m = apply_transforms(im)\n",
        "  m = m.cuda()\n",
        "\n",
        "  Oci = X(m).max(1)[0]\n",
        "  print(Oci)\n",
        "  \n",
        "  avg_drop_sc.append(((max(0, Yci - Oci))/(Yci)))\n",
        "  print(((max(0, Yci - Oci))/(Yci)))\n",
        "  \n",
        "  if Yci < Oci:\n",
        "    avg_iic_sc.append(1)\n",
        "  else:\n",
        "    avg_iic_sc.append(0)    \n",
        "\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjEpwcBwkVFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in f:\n",
        "  get_activation_map(filepath + i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwg8c_OtrkxN",
        "colab_type": "code",
        "outputId": "631ca98c-0333-4e36-dd9f-370c89af669f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"ScoreCAM results: \")\n",
        "\n",
        "print(\"Average Drop: {}\".format(sum(avg_drop_sc) / len(avg_drop_sc))\n",
        "print(\"Average IIC(Increase in Confidence: {}\".format(sum(avg_iic_sc) / len(avg_iic_sc)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ScoreCAM results: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R0xr1kjroW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Smooth ScoreCAM results: \")\n",
        "\n",
        "print(\"Average Drop: {}\".format(sum(avg_drop) / len(avg_drop)) # LOWER THE BETTER\n",
        "print(\"Average IIC(Increase in Confidence: {}\".format(sum(avg_iic) / len(avg_iic))) # HIGHER THE BETTER"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}